{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 customized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from peft import LoraConfig, PeftModel, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight\n",
      "0.bias\n",
      "2.weight\n",
      "2.bias\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Linear(10, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 2)\n",
    ")\n",
    "for name, params in net.named_parameters():\n",
    "    print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(target_modules=['0'])\n",
    "peft_model = get_peft_model(net, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Sequential(\n",
       "      (0): lora.Linear(\n",
       "        (base_layer): Linear(in_features=10, out_features=20, bias=True)\n",
       "        (lora_dropout): ModuleDict(\n",
       "          (default): Identity()\n",
       "        )\n",
       "        (lora_A): ModuleDict(\n",
       "          (default): Linear(in_features=10, out_features=8, bias=False)\n",
       "        )\n",
       "        (lora_B): ModuleDict(\n",
       "          (default): Linear(in_features=8, out_features=20, bias=False)\n",
       "        )\n",
       "        (lora_embedding_A): ParameterDict()\n",
       "        (lora_embedding_B): ParameterDict()\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=20, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.weight\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.weight\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.weight\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.weight\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.bias\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.weight\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.weight\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.bias\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.weight\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.weight\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.weight\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.weight\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.weight\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.weight\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.weight\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.weight\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.weight\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.weight\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q.weight\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k.weight\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v.weight\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.weight\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.bias\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q.weight\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.weight\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.weight\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.weight\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.bias\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_q.weight\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_k.weight\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_v.weight\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.weight\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.bias\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_q.weight\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k.weight\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v.weight\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.weight\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.bias\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_q.weight\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_k.weight\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_v.weight\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q.weight\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k.weight\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v.weight\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_q.weight\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_k.weight\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_v.weight\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.weight\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.bias\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_q.weight\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k.weight\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v.weight\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.weight\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.bias\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_q.weight\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_k.weight\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_v.weight\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.weight\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.weight\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias\n"
     ]
    }
   ],
   "source": [
    "from diffusers import UNet2DConditionModel\n",
    "net = UNet2DConditionModel(sample_size=64)\n",
    "for name, params in net.named_parameters():\n",
    "    if 'to' in name:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_lora_config = LoraConfig(r=8, init_lora_weights=\"gaussian\", target_modules=[\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"])\n",
    "peft_model = get_peft_model(net, unet_lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,725,440 || all params: 874,025,924 || trainable%: 0.19741290877317272\n"
     ]
    }
   ],
   "source": [
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 multi-lora switch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create 2 lora modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Linear(10, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "config0 = LoraConfig(target_modules=['0'])\n",
    "model0 = get_peft_model(net, config0)\n",
    "model0.save_pretrained(\"./lora0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config2 = LoraConfig(target_modules=['2'])\n",
    "model2 = get_peft_model(net, config2)\n",
    "model2.save_pretrained(\"./lora2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Sequential(\n",
       "      (0): lora.Linear(\n",
       "        (base_layer): Linear(in_features=10, out_features=20, bias=True)\n",
       "        (lora_dropout): ModuleDict(\n",
       "          (default): Identity()\n",
       "        )\n",
       "        (lora_A): ModuleDict(\n",
       "          (default): Linear(in_features=10, out_features=8, bias=False)\n",
       "        )\n",
       "        (lora_B): ModuleDict(\n",
       "          (default): Linear(in_features=8, out_features=20, bias=False)\n",
       "        )\n",
       "        (lora_embedding_A): ParameterDict()\n",
       "        (lora_embedding_B): ParameterDict()\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): lora.Linear(\n",
       "        (base_layer): Linear(in_features=20, out_features=2, bias=True)\n",
       "        (lora_dropout): ModuleDict(\n",
       "          (default): Identity()\n",
       "        )\n",
       "        (lora_A): ModuleDict(\n",
       "          (default): Linear(in_features=20, out_features=8, bias=False)\n",
       "        )\n",
       "        (lora_B): ModuleDict(\n",
       "          (default): Linear(in_features=8, out_features=2, bias=False)\n",
       "        )\n",
       "        (lora_embedding_A): ParameterDict()\n",
       "        (lora_embedding_B): ParameterDict()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Sequential(\n",
       "      (0): lora.Linear(\n",
       "        (base_layer): Linear(in_features=10, out_features=20, bias=True)\n",
       "        (lora_dropout): ModuleDict(\n",
       "          (lora0): Identity()\n",
       "        )\n",
       "        (lora_A): ModuleDict(\n",
       "          (lora0): Linear(in_features=10, out_features=8, bias=False)\n",
       "        )\n",
       "        (lora_B): ModuleDict(\n",
       "          (lora0): Linear(in_features=8, out_features=20, bias=False)\n",
       "        )\n",
       "        (lora_embedding_A): ParameterDict()\n",
       "        (lora_embedding_B): ParameterDict()\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=20, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Linear(10, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 2)\n",
    ")\n",
    "lora_model = PeftModel.from_pretrained(net, \"./lora0\", adapter_name=\"lora0\")\n",
    "lora_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['base_model.model.0.base_layer.weight', 'base_model.model.0.base_layer.bias', 'base_model.model.0.lora_A.lora0.weight', 'base_model.model.0.lora_B.lora0.weight', 'base_model.model.2.base_layer.weight', 'base_model.model.2.base_layer.bias'], unexpected_keys=['base_model.model.0.lora_A.lora2.weight', 'base_model.model.0.lora_B.lora2.weight'])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model.load_adapter(\"./lora2\", adapter_name=\"lora2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.0.base_layer.weight\n",
      "base_model.model.0.base_layer.bias\n",
      "base_model.model.0.lora_A.lora0.weight\n",
      "base_model.model.0.lora_B.lora0.weight\n",
      "base_model.model.2.base_layer.weight\n",
      "base_model.model.2.base_layer.bias\n",
      "base_model.model.2.lora_A.lora2.weight\n",
      "base_model.model.2.lora_B.lora2.weight\n"
     ]
    }
   ],
   "source": [
    "for name, params in lora_model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lora0'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model.active_adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lora2'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model.set_adapter(\"lora2\")\n",
    "lora_model.active_adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.0.base_layer.weight Parameter containing:\n",
      "tensor([[ 0.0604,  0.0137, -0.2185,  0.1296, -0.0280,  0.2921,  0.1835, -0.0641,\n",
      "          0.1377,  0.2154],\n",
      "        [ 0.0983,  0.0188, -0.2438, -0.2738,  0.1732,  0.2957,  0.2143,  0.1302,\n",
      "          0.0398,  0.2023],\n",
      "        [-0.0704,  0.1617,  0.0558,  0.2920,  0.1776, -0.1266, -0.2570,  0.1677,\n",
      "         -0.2258, -0.1908],\n",
      "        [ 0.0992,  0.2700, -0.2348, -0.1692,  0.2895, -0.2877,  0.0026, -0.1235,\n",
      "         -0.1730, -0.2905],\n",
      "        [-0.0129, -0.0434,  0.0714, -0.2570,  0.2909,  0.0829,  0.0987, -0.1282,\n",
      "          0.0068, -0.0956],\n",
      "        [-0.0539, -0.0901,  0.0095,  0.2184, -0.0552,  0.3044,  0.1577,  0.0090,\n",
      "         -0.0753,  0.2160],\n",
      "        [ 0.1120,  0.1791,  0.0638,  0.0354, -0.2394,  0.2979, -0.1689,  0.0792,\n",
      "          0.0664,  0.1228],\n",
      "        [ 0.2668,  0.0697, -0.0941, -0.1233,  0.2935, -0.1532,  0.1243,  0.0610,\n",
      "          0.1191, -0.2520],\n",
      "        [-0.1284, -0.2475, -0.1974,  0.2049,  0.1029,  0.2042, -0.0631, -0.0983,\n",
      "          0.2622,  0.1241],\n",
      "        [-0.0203,  0.0637, -0.1417, -0.2146, -0.0195, -0.0207,  0.1487, -0.1365,\n",
      "         -0.2630,  0.2905],\n",
      "        [-0.1231,  0.1799, -0.1807, -0.1439,  0.1139, -0.1954, -0.2511,  0.3029,\n",
      "         -0.3086,  0.0787],\n",
      "        [ 0.1203,  0.1446, -0.3000, -0.1938, -0.2003,  0.1508, -0.2014,  0.0123,\n",
      "         -0.0913,  0.2540],\n",
      "        [-0.2370,  0.2312,  0.1300, -0.0322,  0.0961, -0.0923,  0.0363, -0.1085,\n",
      "          0.1655,  0.1946],\n",
      "        [-0.0462,  0.0096, -0.1641,  0.2549,  0.0837, -0.3042, -0.1667, -0.2865,\n",
      "          0.0147,  0.2720],\n",
      "        [ 0.0500,  0.1062,  0.2515,  0.2703, -0.0153, -0.2645, -0.1941,  0.0358,\n",
      "         -0.1215,  0.0369],\n",
      "        [ 0.0689,  0.0635, -0.2161,  0.2442,  0.2517,  0.1344, -0.0476, -0.3092,\n",
      "          0.0128, -0.0230],\n",
      "        [-0.0143,  0.0217, -0.0915,  0.0647, -0.1520,  0.0064, -0.1364, -0.0538,\n",
      "         -0.0508, -0.3159],\n",
      "        [ 0.2518,  0.1500,  0.1476,  0.0403,  0.0683,  0.0629,  0.2462,  0.3132,\n",
      "          0.0703, -0.1011],\n",
      "        [-0.0817,  0.0029, -0.2186,  0.1612,  0.2708,  0.2326,  0.1231,  0.1931,\n",
      "          0.2435, -0.1474],\n",
      "        [-0.2152,  0.1550,  0.2690,  0.2745, -0.0172, -0.0438,  0.2382,  0.0132,\n",
      "          0.2231, -0.0272]])\n",
      "base_model.model.0.base_layer.bias Parameter containing:\n",
      "tensor([-0.0959,  0.2584, -0.0122, -0.2662, -0.3146, -0.0692, -0.2982, -0.2930,\n",
      "         0.2597, -0.1660, -0.3162, -0.2278,  0.0954, -0.0208, -0.0643,  0.2077,\n",
      "         0.2803, -0.2947,  0.0264,  0.0472])\n",
      "base_model.model.0.lora_A.lora0.weight Parameter containing:\n",
      "tensor([[-0.1947, -0.2801, -0.1451,  0.2264, -0.2111, -0.2125,  0.0757, -0.2422,\n",
      "         -0.1875,  0.1383],\n",
      "        [-0.3000, -0.2031,  0.2676,  0.2354,  0.1500, -0.0533,  0.1593, -0.1379,\n",
      "          0.1095, -0.2350],\n",
      "        [-0.2079, -0.2530,  0.0531, -0.1915, -0.0011,  0.1384,  0.0949, -0.2043,\n",
      "         -0.0357,  0.0817],\n",
      "        [ 0.1536, -0.2869, -0.2629, -0.1283, -0.0963, -0.2945, -0.2477,  0.1263,\n",
      "          0.0685,  0.1800],\n",
      "        [-0.0644, -0.2860,  0.1786, -0.2930,  0.2989,  0.0361, -0.0056,  0.1987,\n",
      "         -0.2813, -0.0323],\n",
      "        [ 0.1562,  0.2148, -0.0027, -0.0186,  0.1023, -0.2468, -0.1437,  0.2212,\n",
      "         -0.2738, -0.1330],\n",
      "        [ 0.1788,  0.2154, -0.0041, -0.0607, -0.0246,  0.0008, -0.1101,  0.0229,\n",
      "         -0.0872,  0.1473],\n",
      "        [-0.2793,  0.1461, -0.0042,  0.1153, -0.1808,  0.1955, -0.3137, -0.1844,\n",
      "         -0.2023, -0.1162]])\n",
      "base_model.model.0.lora_B.lora0.weight Parameter containing:\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "base_model.model.2.base_layer.weight Parameter containing:\n",
      "tensor([[ 0.1938,  0.1838,  0.1521,  0.0769, -0.0288,  0.2114,  0.0427,  0.1711,\n",
      "          0.1800, -0.1094,  0.1347,  0.1339,  0.1819, -0.2232, -0.2000,  0.0206,\n",
      "          0.1295,  0.1882,  0.1074, -0.2057],\n",
      "        [-0.1638,  0.0583,  0.1746,  0.1831, -0.1552,  0.2050,  0.0488,  0.0471,\n",
      "          0.1920,  0.1969,  0.0888,  0.1003,  0.0714,  0.2025,  0.1460, -0.2063,\n",
      "         -0.1660,  0.0606,  0.0806, -0.0955]])\n",
      "base_model.model.2.base_layer.bias Parameter containing:\n",
      "tensor([-0.2209,  0.2138])\n",
      "base_model.model.2.lora_A.lora2.weight Parameter containing:\n",
      "tensor([[-7.6357e-02, -1.6593e-01, -2.0032e-01, -1.9701e-01, -2.0480e-01,\n",
      "         -8.3795e-02,  9.8867e-02, -6.0036e-02,  1.8629e-01,  1.3379e-01,\n",
      "          1.9363e-01, -9.3454e-02, -1.8906e-01, -1.2174e-01, -5.5169e-02,\n",
      "         -1.4831e-01,  6.3155e-02, -1.9889e-01, -6.6916e-03,  8.9771e-02],\n",
      "        [ 8.1043e-02,  5.3034e-02, -1.7356e-01, -6.7297e-02, -2.2348e-01,\n",
      "          1.3136e-02, -3.1876e-02,  2.0762e-01,  1.2926e-01, -2.0540e-01,\n",
      "         -2.1999e-01, -9.9123e-02, -4.5208e-02,  8.3380e-05,  3.0435e-02,\n",
      "         -1.4986e-03, -1.0344e-01, -1.4473e-01, -1.1871e-01,  1.4231e-01],\n",
      "        [ 1.6108e-01, -3.1572e-02, -8.9190e-02, -1.9155e-01,  5.9512e-03,\n",
      "          1.7807e-01, -1.9966e-01,  6.9526e-02,  2.1429e-01, -6.5926e-02,\n",
      "         -1.2155e-01, -5.4152e-02, -1.8037e-01, -8.0323e-02, -4.2971e-02,\n",
      "         -1.5004e-01,  1.6116e-01, -6.5458e-02, -1.9456e-01, -1.7929e-01],\n",
      "        [ 1.4702e-01,  2.6988e-02, -9.5930e-02, -1.1135e-01,  1.4129e-01,\n",
      "          1.9638e-01, -1.2257e-01, -3.0720e-02, -4.1379e-02, -1.4972e-01,\n",
      "         -2.0187e-01, -1.6183e-01, -3.3349e-03, -9.0890e-03,  3.4365e-02,\n",
      "         -1.5171e-01, -4.3941e-02,  9.0129e-02,  1.2338e-02,  1.3210e-01],\n",
      "        [-2.3466e-02,  1.9138e-01,  1.8943e-01,  6.0509e-02, -1.2403e-01,\n",
      "          8.2509e-02,  1.1117e-01,  1.8712e-01,  9.4138e-02,  1.8365e-01,\n",
      "          1.2280e-01,  1.4863e-01,  6.6884e-02,  1.2273e-01, -1.4204e-01,\n",
      "          1.9845e-01,  1.3885e-01,  1.2073e-01,  1.0927e-01, -1.6971e-01],\n",
      "        [-1.9645e-01,  1.0016e-01,  1.4093e-01, -5.3557e-02, -1.8592e-02,\n",
      "         -9.1455e-02,  3.3978e-02, -1.0000e-01,  4.3621e-02, -6.7871e-02,\n",
      "          2.0920e-01,  7.9935e-03, -2.1283e-01,  9.1270e-02,  2.1139e-01,\n",
      "         -2.7619e-02, -1.3178e-01,  1.2492e-01,  5.3618e-02, -8.7264e-02],\n",
      "        [ 2.0088e-01, -1.3918e-01,  1.0275e-01, -7.8935e-02,  1.7287e-01,\n",
      "         -1.0268e-01, -1.7109e-01,  1.7861e-01, -1.4571e-01, -9.5637e-02,\n",
      "          1.7726e-01,  1.3996e-02,  8.9139e-02,  9.1015e-02, -1.9018e-01,\n",
      "         -1.1417e-01, -1.2115e-03, -2.4991e-02, -1.7993e-01, -1.6044e-01],\n",
      "        [ 1.1259e-01,  1.6141e-01, -1.0934e-01, -4.9785e-02,  7.8620e-02,\n",
      "         -4.7440e-02, -7.2928e-02,  1.3193e-01,  1.0489e-01, -1.1494e-01,\n",
      "         -1.8567e-01, -3.1758e-02,  6.3123e-02,  9.4525e-02,  1.6958e-01,\n",
      "         -2.1395e-01,  9.2160e-02,  4.5849e-02,  5.0052e-02,  1.0179e-01]],\n",
      "       requires_grad=True)\n",
      "base_model.model.2.lora_B.lora2.weight Parameter containing:\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name, params in lora_model.named_parameters():\n",
    "    print(name, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 diable lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1976,  0.2378]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1, 10)\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1976,  0.2378]])\n"
     ]
    }
   ],
   "source": [
    "with lora_model.disable_adapter():\n",
    "    print(lora_model(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IMP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
