{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import *\n",
    "datasets = load_dataset(\"madao33/new-title-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'content'],\n",
       "        num_rows: 5850\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['title', 'content'],\n",
       "        num_rows: 1679\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 different ways of slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'content'],\n",
       "    num_rows: 90\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"madao33/new-title-chinese\", split=\"train[10:100]\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'content'],\n",
       "    num_rows: 2925\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"madao33/new-title-chinese\", split=\"train[:50%]\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Dataset({\n",
       "     features: ['title', 'content'],\n",
       "     num_rows: 2925\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['title', 'content'],\n",
       "     num_rows: 2925\n",
       " })]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"madao33/new-title-chinese\", split=[\"train[:50%]\", \"train[50%:]\"])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'content'],\n",
       "        num_rows: 5265\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['title', 'content'],\n",
       "        num_rows: 585\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datasets[\"train\"]\n",
    "dataset.train_test_split(test_size=0.1) # stratifiy_by_column = \"label\" can balance the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'content'],\n",
       "    num_rows: 5850\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Inspecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': ['望海楼美国打“台湾牌”是危险的赌博',\n",
       "  '大力推进高校治理能力建设',\n",
       "  '坚持事业为上选贤任能',\n",
       "  '“大朋友”的话儿记心头',\n",
       "  '用好可持续发展这把“金钥匙”'],\n",
       " 'content': ['近期，美国国会众院通过法案，重申美国对台湾的承诺。对此，中国外交部发言人表示，有关法案严重违反一个中国原则和中美三个联合公报规定，粗暴干涉中国内政，中方对此坚决反对并已向美方提出严正交涉。\\n事实上，中国高度关注美国国内打“台湾牌”、挑战一中原则的危险动向。近年来，作为“亲台”势力大本营的美国国会动作不断，先后通过“与台湾交往法”“亚洲再保证倡议法”等一系列“挺台”法案，“2019财年国防授权法案”也多处触及台湾问题。今年3月，美参院亲台议员再抛“台湾保证法”草案。众院议员继而在4月提出众院版的草案并在近期通过。上述法案的核心目标是强化美台关系，并将台作为美“印太战略”的重要伙伴。同时，“亲台”议员还有意制造事端。今年2月，5名共和党参议员致信众议院议长，促其邀请台湾地区领导人在国会上发表讲话。这一动议显然有悖于美国与台湾的非官方关系，其用心是实质性改变美台关系定位。\\n上述动向出现并非偶然。在中美建交40周年之际，两国关系摩擦加剧，所谓“中国威胁论”再次沉渣泛起。美国对华认知出现严重偏差，对华政策中负面因素上升，保守人士甚至成立了“当前中国威胁委员会”。在此背景下，美国将台海关系作为战略抓手，通过打“台湾牌”在双边关系中增加筹码。特朗普就任后，国会对总统外交政策的约束力和塑造力加强。其实国会推动通过涉台法案对行政部门不具约束力，美政府在2018年并未提升美台官员互访级别，美军舰也没有“访问”台湾港口，保持着某种克制。但从美总统签署国会通过的法案可以看出，国会对外交产生了影响。立法也为政府对台政策提供更大空间。\\n然而，美国需要认真衡量打“台湾牌”成本。首先是美国应对危机的代价。美方官员和学者已明确发出警告，美国卷入台湾问题得不偿失。美国学者曾在媒体发文指出，如果台海爆发危机，美国可能需要“援助”台湾，进而导致新的冷战乃至与中国大陆的冲突。但如果美国让台湾自己面对，则有损美国的信誉，影响美盟友对同盟关系的支持。其次是对中美关系的危害。历史证明，中美合则两利、斗则两伤。中美关系是当今世界最重要的双边关系之一，保持中美关系的稳定发展，不仅符合两国和两国人民的根本利益，也是国际社会的普遍期待。美国蓄意挑战台湾问题的底线，加剧中美关系的复杂性和不确定性，损害两国在重要领域合作，损人又害己。\\n美国打“台湾牌”是一场危险的赌博。台湾问题是中国核心利益，中国政府和人民决不会对此坐视不理。中国敦促美方恪守一个中国原则和中美三个联合公报规定，阻止美国会审议推进有关法案，妥善处理涉台问题。美国悬崖勒马，才是明智之举。\\n（作者系中国国际问题研究院国际战略研究所副所长）',\n",
       "  '在推进“双一流”高校建设进程中，我们要紧紧围绕为党育人、为国育才，找准问题、破解难题，以一流意识和担当精神，大力推进高校的治理能力建设。\\n增强政治引领力。坚持党对高校工作的全面领导，始终把政治建设摆在首位，增强校党委的政治领导力，全面推进党的建设各项工作。落实立德树人根本任务，把培养社会主义建设者和接班人放在中心位置。紧紧抓住思想政治工作这条生命线，全面加强师生思想政治工作，推进“三全育人”综合改革，将思想政治工作贯穿学校教育管理服务全过程，努力让学生成为德才兼备、全面发展的人才。\\n提升人才聚集力。人才是创新的核心要素，创新驱动本质上是人才驱动。要坚持引育并举，建立绿色通道，探索知名专家举荐制，完善“一事一议”支持机制。在大力支持自然科学人才队伍建设的同时，实施哲学社会科学人才工程。立足实际，在条件成熟的学院探索“一院一策”改革。创新科研组织形式，为人才成长创设空间，建设更加崇尚学术、更加追求卓越、更加关爱学生、更加担当有为的学术共同体。\\n培养学生竞争力。遵循学生成长成才的规律培育人才，着力培养具有国际竞争力的拔尖创新人才和各类专门人才，使优势学科、优秀教师、优质资源、优良环境围绕立德树人的根本任务配置。淘汰“水课”，打造“金课”，全力打造世界一流本科教育。深入推进研究生教育综合改革，加强事关国家重大战略的高精尖急缺人才培养，建设具有国际竞争力的研究生教育。\\n激发科技创新力。在国家急需发展的领域挑大梁，就要更加聚焦科技前沿和国家需求，狠抓平台建设，包括加快牵头“武汉光源”建设步伐，积极参与国家实验室建设，建立校级大型科研仪器设备共享平台。关键核心技术领域“卡脖子”问题，归根结底是基础科学研究薄弱。要加大基础研究的支持力度，推进理论、技术和方法创新，鼓励支持重大原创和颠覆性技术创新，催生一批高水平、原创性研究成果。\\n发展社会服务力。在贡献和服务中体现价值，推动合作共建、多元投入的格局，大力推进政产学研用结合，强化科技成果转移转化及产业化。探索校城融合发展、校地联动发展的新模式，深度融入地方创新发展网络，为地方经济社会发展提供人才支撑，不断拓展和优化社会服务网络。\\n涵育文化软实力。加快体制机制改革，优化学校、学部、学院三级评审机制，充分发挥优秀学者特别是德才兼备的年轻学者在学术治理中的重要作用。牢固树立一流意识、紧紧围绕一流目标、认真执行一流标准，让成就一流事业成为普遍追求和行动自觉。培育具有强大凝聚力的大学文化，营造积极团结、向上向善、干事创业的氛围，让大学成为吸引和留住一大批优秀人才建功立业的沃土，让敢干事、肯干事、能干事的人有更多的荣誉感和获得感。\\n建设中国特色、世界一流大学不是等得来、喊得来的，而是脚踏实地拼出来、干出来的。对标一流，深化改革，坚持按章程办学，构建以一流质量标准为核心的制度规范体系，扎实推进学校综合改革，探索更具活力、更富效率的管理体制和运行机制，我们就一定能构建起具有中国特色的现代大学治理体系，进一步提升管理服务水平和工作效能。\\n（作者系武汉大学校长）',\n",
       "  '育才造士，为国之本。党的干部是党和国家事业的中坚力量。习近平总书记深刻指出，“历史和现实都表明，一个政党、一个国家能不能不断培养出优秀领导人才，在很大程度上决定着这个政党、这个国家的兴衰存亡。”新修订的《党政领导干部选拔任用工作条例》，坚持以推进伟大事业为导向，将“事业为上、人岗相适、人事相宜”作为一条重要原则，为做好新时代选人用人工作、建设忠诚干净担当的高素质专业化干部队伍，进一步指明了正确方向。\\n党的干部总是与党的事业紧紧连在一起，伟大事业需要高素质干部，干部要在事业发展中锻炼成长。党的十八大以来，党和国家事业之所以取得历史性成就、发生历史性变革，根本原因是有以习近平同志为核心的党中央坚强领导，有习近平新时代中国特色社会主义思想的科学指引，同时也与广大干部奋发有为、改革创新、干事创业、担当奉献密不可分。当前，中国特色社会主义进入新时代，站在新的历史起点上，我们党要肩负起新的历史使命，必须贯彻新时代党的组织路线，坚持从党和人民事业需要出发选干部、用干部，突出实践实干选贤能，坚持有为有位聚英才，真正做到事业发展需要什么样的人就用什么样的人，什么样的人最合适就选什么样的人。\\n为官择人者治，为人择官者乱。新修订的《干部任用条例》，通篇贯穿事业导向、事业要求，在提出深入考察干部政治素质、道德品行、作风素养、廉政情况的同时，强调要突出工作实绩、履职尽责等方面的考察，大力选拔敢于负责、勇于担当、善于作为、实绩突出的优秀干部。贯彻落实条例，必须正确把握事业发展需要和干部成长进步的关系，知事识人、依事择人、精准选人，把善于统筹推进“五位一体”总体布局和协调推进“四个全面”战略布局，贯彻落实新发展理念、推进高质量发展、深化供给侧结构性改革、打好“三大攻坚战”的优秀干部及时发现出来、合理使用起来，进而示范引领更多干部勇于担当作为，心无旁骛干事业、坚定信心促发展。\\n不拒众流，方为江海。五湖四海的事业，需要五湖四海的人来干。新修订的《干部任用条例》，鲜明提出要注意从企业、高等学校、科研院所等单位以及社会组织中发现选拔党政领导干部，对推动国有企事业单位、社会组织干部人才及时进入党政机关作出制度性安排，这是我们党选人用人成功经验的深刻总结和运用。贯彻落实条例，就要坚持干部工作一盘棋，进一步开阔视野、拓宽渠道，放眼各条战线、各个领域、各个行业、各个层级，充分盘活干部资源，广开进贤之路。要坚持公道正派、公正用人，选拔干部论能力、看水平、凭实绩，而不能搞平衡照顾、论资排辈、降格以求，更不能搞小圈子、任人唯亲，在少数人中选人。要突出政治过硬、本领高强，深入考察识别干部的专业能力、专业素养、专业精神。坚持立足当前、着眼长远，注重发现培养和选拔使用在改革发展稳定一线特别是在重大斗争中经过磨砺的优秀年轻干部，为党和国家事业发展源源不断地注入生机和活力。要坚持严管和厚爱结合、激励和约束并重，认真贯彻习近平总书记关于“三个区分开来”的重要要求，宽容干部在改革创新中的失误错误，保护干部干事创业的积极性。对那些符合有关规定给予容错的干部，要认真落实条例的要求，给予客观公正对待，为他们重整旗鼓、轻装上阵、贡献才智搭建平台。\\n宏伟的事业，离不开高素质专业化的干部。各级党委（党组）及其组织人事部门，要以贯彻落实新修订的《干部任用条例》为契机，坚持党的原则第一、党的事业第一、人民利益第一，大力选拔党和人民需要的好干部，为推动中国特色社会主义伟大事业乘风破浪、不断前进提供坚强组织保证。\\n《 人民日报 》（ 2019年03月20日\\xa0\\xa0 06 版）\\n',\n",
       "  '今天（6月1日）是“六一”国际儿童节。公园里，跃动着孩子们快乐嬉戏的身影；校园里，回荡着孩子们清脆悦耳的歌声。\\n“少年智则国智，少年富则国富，少年强则国强”。少年儿童是祖国的未来，是民族的希望。少年儿童的茁壮成长，始终牵动着习近平总书记的心。作为孩子们的“大朋友”，他勉励孩子们珍惜时光，努力学习，“像海绵吸水一样学习知识”；他告诉孩子们，有梦想，还要脚踏实地，好好读书，才能梦想成真；他寄语孩子们，多想一想，就会促使自己多做一做，日积月累，自己身上的好思想、好品德就会越来越多了。\\n话语暖人，寄望殷殷。\\n志向是人生的航标。一个人要做出一番成就，就要有自己的志向。所以，我们要教育孩子们从小树立远大志向，不能做无舵的小舟，漫无方向。\\n做人要以品德为先。品德就像大树的根脉，根扎得深、长得正，人生才会有高度。所以，我们要注重孩子们的品德修养，引导他们从人生一开始就要扣好“第一粒扣子”。\\n幸福不是免费午餐。实现梦想、创造幸福，需要一步一个脚印，一点一滴积累。所以，我们要教育孩子们从小珍惜时间，学知识，长本领，勤思考，多实践，将来才能让人生出彩、为社会添彩。\\n让我们把“大朋友”的话牢记心头，引导孩子们在金色的童年中放飞梦想、播种希望，让他们健康成长，成为建设祖国的栋梁之才。',\n",
       "  '11月22日晚，习近平主席在北京以视频方式出席二十国集团领导人第十五次峰会第二阶段会议，重点阐述关于可持续发展问题的看法。他指出，要打造包容性、可持续、有韧性的未来，不断推进全球减贫事业至关重要。面对新冠肺炎疫情冲击，我们比以往任何时候都更需要拿出切实举措。\\n当前，世纪疫情和百年变局交织、全球经济陷入严重衰退、世界进入动荡变革期。破解一系列全球性问题，可持续发展是一把 “金钥匙”。能不能用好这把“金钥匙”，关乎各国眼前乃至长远利益，考验人类智慧与道义。\\n用好这把“金钥匙”需要共商共建。一个人的努力是加法，一个团队的努力是乘法。面对重重挑战，二十国集团应加强沟通交流、团结合作，以落实联合国2030年可持续发展议程为引领，加大支持发展中国家克服疫情，推动经济复苏和增长，缩小数字鸿沟，努力营造良好的国际经济环境。同时，减少碳排放，促进绿色低碳发展，合力应对气候变化和生态环境恶化等全球性挑战。\\n用好这把“金钥匙”需要共创共享。消除贫困是人类的共同使命。改革开放40多年来，中国7亿多人摆脱贫困，对世界减贫贡献率超过70%。时下，中国即将提前10年实现《联合国2030年可持续发展议程》减贫目标，完成这项对中华民族、对人类社会都具有重大意义的伟业。“中国式扶贫”作为全球可持续发展的典范，为世界各国尤其是贫困国家提供了参考方案。作为二十国集团重要一员，中国愿与世界分享“中国式扶贫”的经验做法，提供减贫的“中国方案”，同各国一道，合力建设远离贫困、共同发展的美好世界。\\n“万物并育而不相害，道并行而不相悖。”今天的世界，人类交往比过去任何时候都更深入、更广泛、更紧密。面对新冠疫情对世界政治和经济生活带来的巨大冲击，世界各国坚持用好可持续发展这把“金钥匙”，合作共治、开放共享、互利共赢，必将穿过全球不确定性迷雾，走出一条包容性、可持续、有韧性的康庄大道。（文字：宋燕 海报：张倩）']}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['望海楼美国打“台湾牌”是危险的赌博', '大力推进高校治理能力建设']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"]['title'][:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['title', 'content']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': Value(dtype='string', id=None),\n",
       " 'content': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"].features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48fbd2cf5a644db8be12ae7641dbf99f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/5850 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filtered_dataset = datasets[\"train\"].filter(lambda example: '中国' in example['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['聚焦两会，世界探寻中国成功秘诀',\n",
       " '望海楼中国经济的信心来自哪里',\n",
       " '“中国奇迹”助力世界减贫跑出加速度',\n",
       " '和音瞩目历史交汇点上的中国',\n",
       " '中国风采感染世界']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_dataset['title'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prefix(example):\n",
    "    example['title'] = 'PREFIX: ' + example['title']\n",
    "    return example\n",
    "\n",
    "prefix_dataset = datasets[\"train\"].map(add_prefix)\n",
    "prefix_dataset['title'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "def process_function(example):\n",
    "    inputs = tokenizer(example['content'], truncation=True, max_length=512)\n",
    "    labels = tokenizer(example['title'], truncation=True, max_length=512)\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_dataset = datasets.map(process_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'content', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 5850\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['title', 'content', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1679\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## multi-processing\n",
    "process_dataset = datasets.map(process_function, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_dataset = datasets.map(process_function, batched=True, remove_columns=datasets['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 5850\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1679\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save and load\n",
    "\n",
    "```\n",
    "process_dataset.save_to_disk(PATH)\n",
    "process_dataset.load_from_disk(PATH)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Customized Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 from memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['CN', 'EN'],\n",
       "    num_rows: 2\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    {\"CN\": \"我爱你\", \"EN\": \"I love you\"},\n",
    "    {\"CN\": \"我恨你\", \"EN\": \"I hate you\"},\n",
    "]\n",
    "dataset = Dataset.from_list(data)\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 load from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('csv', data_files='data/ChnSentiCorp_htl_all.csv')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatset = Dataset.from_csv('data/ChnSentiCorp_htl_all.csv')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 load from directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('csv', data_dir = 'data/csv_chunks/')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 load with scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import json\n",
    "import datasets\n",
    "from datasets import DownloadManager, DatasetInfo\n",
    "\n",
    "\n",
    "class CMRC2018TRIAL(datasets.GeneratorBasedBuilder):\n",
    "\n",
    "    def _info(self) -> DatasetInfo:\n",
    "        \"\"\"\n",
    "            info方法, 定义数据集的信息,这里要对数据的字段进行定义.\n",
    "                - description\n",
    "                - features\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "    def _split_generators(self, dl_manager: DownloadManager):\n",
    "        \"\"\"\n",
    "            返回datasets.SplitGenerator\n",
    "            涉及两个参数: name和gen_kwargs\n",
    "            name: 指定数据集的划分\n",
    "            gen_kwargs: 指定要读取的文件的路径, 与_generate_examples的入参数一致\n",
    "        :param dl_manager:\n",
    "        :return: [ datasets.SplitGenerator ]\n",
    "        \"\"\"\n",
    "\n",
    "    def _generate_examples(self, filepath):\n",
    "        \"\"\"\n",
    "            生成具体的样本, 使用yield\n",
    "            需要额外指定key, id从0开始自增就可以\n",
    "        :param filepath:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import *\n",
    "# be careful with the dataset path here; I hard coded it in the script so that it can be run in the notebook (respecting the relative path)\n",
    "dataset = load_dataset('scripts/04_load_script.py', split = 'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dataset With DataCollator\n",
    "\n",
    "Dynamically padding samples within a batch so that they have the same shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'review'],\n",
       "    num_rows: 7765\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset('csv', data_files='data/ChnSentiCorp_htl_all.csv', split='train')\n",
    "# remove the na rows\n",
    "dataset = dataset.filter(lambda example: example['review'] is not None)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db482703ef864a378d93a32085d0877e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef131f7d07764a86a46d9f10b8cdb61c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/624 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3ef23e641d94f4fb6eaff742aec11b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dcf9b5124204f4dabc84942b2ae735a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/269k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "def process_function(example):\n",
    "    # 动态填充 短的samples\n",
    "    inputs = tokenizer(example['review'], truncation=True, max_length=128)\n",
    "    inputs[\"labels\"] = example['label']\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.map(process_function, batched=True, remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "samples = tokenized_dataset[:4]\n",
    "print(len(samples['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[52, 29, 44, 128]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(sen) for sen in samples['input_ids']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(tokenized_dataset, collate_fn=collator, shuffle=True, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 128])\n",
      "torch.Size([4, 93])\n",
      "torch.Size([4, 128])\n",
      "torch.Size([4, 109])\n",
      "torch.Size([4, 128])\n",
      "torch.Size([4, 63])\n",
      "torch.Size([4, 128])\n",
      "torch.Size([4, 128])\n",
      "torch.Size([4, 128])\n",
      "torch.Size([4, 86])\n",
      "torch.Size([4, 128])\n"
     ]
    }
   ],
   "source": [
    "num = 0\n",
    "for batch in train_dataloader:\n",
    "    print(batch['input_ids'].shape)\n",
    "    num += 1\n",
    "    if num > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. fine tuning with `Datasets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'review'],\n",
       "    num_rows: 7765\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset('csv', data_files='data/ChnSentiCorp_htl_all.csv', split='train')\n",
    "dataset = dataset.filter(lambda example: example['review'] is not None)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 train test splot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'review'],\n",
       "        num_rows: 6988\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'review'],\n",
       "        num_rows: 777\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split the dataset\n",
    "datasets = dataset.train_test_split(test_size=0.1)\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "checkpoint = \"hfl/rbt3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def process_function(example):\n",
    "    inputs = tokenizer(example['review'], truncation=True, max_length=128)\n",
    "    inputs[\"labels\"] = example['label']\n",
    "    return inputs\n",
    "\n",
    "tokenized_datasets = datasets.map(process_function, batched=True, remove_columns=datasets['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 6988\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 777\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "train_dataloader = DataLoader(tokenized_datasets['train'], collate_fn=collator, shuffle=True, batch_size=32)\n",
    "eval_dataloader = DataLoader(tokenized_datasets['test'], collate_fn=collator, shuffle=False, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 模型和优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/rbt3 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "optimizer = Adam(model.parameters(), lr=2e-5)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.inference_mode():\n",
    "        for batch in eval_dataloader:\n",
    "            if torch.cuda.is_available():\n",
    "                batch = {k: v.cuda() for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            correct += torch.sum(preds == batch['labels'])\n",
    "        acc = correct / len(tokenized_datasets['test'])\n",
    "        return acc\n",
    "\n",
    "\n",
    "def train(num_epoch=1, log_step=100):\n",
    "\n",
    "    global_step = 0\n",
    "    for ep in range(num_epoch):\n",
    "        model.train()\n",
    "        for batch in train_dataloader:\n",
    "            if torch.cuda.is_available():\n",
    "                batch = {k: v.cuda() for k, v in batch.items()}\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            global_step += 1\n",
    "            if global_step % log_step == 0:\n",
    "                print(f\"global_step: {global_step:6d}, loss: {loss.item():.4f}\")\n",
    "            acc = evaluate()\n",
    "        print(f\"epoch: {ep}, acc: {acc:.4f}\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step:    100, loss: 0.2824\n",
      "global_step:    200, loss: 0.4679\n",
      "epoch: 0, acc: 0.9048\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IMP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
