{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.load_from_disk('data/alpaca_data_zh')\n",
    "datasets = ds.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fd02a1b97dc4b06954ad69ef623f1b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21486 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "949aec42398a4fc8a83e3ca916648aad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5372 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ckpt = 'Langboat/bloom-1b4-zh'\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckpt)\n",
    "\n",
    "def process_function(example):\n",
    "    MAX_LENGTH = 256\n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "    instruction = example['instruction']\n",
    "    input_str = example['input']\n",
    "    instruction_input_seq = \"\\n\".join([\"Human: \" + instruction, input_str]).strip() + \"\\n\\n Assistant:\"\n",
    "    response_str = example['output'] + tokenizer.eos_token\n",
    "    tokenized_instruction_input = tokenizer(instruction_input_seq)\n",
    "    tokenized_response = tokenizer(response_str)\n",
    "    input_ids = tokenized_instruction_input['input_ids'] + tokenized_response['input_ids']\n",
    "    attention_mask = tokenized_instruction_input['attention_mask'] + tokenized_response['attention_mask']\n",
    "    labels = [-100] * len(tokenized_instruction_input['input_ids']) + tokenized_response['input_ids']\n",
    "    if len(input_ids) > MAX_LENGTH:\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}\n",
    "\n",
    "tokenized_ds = datasets.map(process_function, remove_columns=ds.column_names)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 1.30 B\n",
      "Model size: 5.21 GB\n",
      "Gradient size: 5.21 GB\n",
      "Optimizer state: 10.42 GB\n",
      "Total size: 20.85 GB\n"
     ]
    }
   ],
   "source": [
    "unit = 1000\n",
    "num_params = sum(params.numel() for params in model.parameters())\n",
    "print(f'Number of parameters: {num_params/unit**3:.2f} B')\n",
    "\n",
    "model_size = num_params * 4 / unit**3\n",
    "print(f'Model size: {model_size:.2f} GB')\n",
    "\n",
    "gradient_size = model_size\n",
    "print(f'Gradient size: {gradient_size:.2f} GB')\n",
    "\n",
    "optimizer_state = model_size * 2\n",
    "print(f'Optimizer state: {optimizer_state:.2f} GB')\n",
    "\n",
    "total_size = model_size + optimizer_state + gradient_size\n",
    "print(f'Total size: {total_size:.2f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lora\n",
    "## PEFT Step1 配置文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "minimum conifg\n",
    "`config = LoraConfig(task_type=TaskType.CAUSAL_LM)`\n",
    "key parameters\n",
    "r: rank\n",
    "lora_alpha: \"step size\", actually the learning rate, scaled as lora_alpha / r\n",
    "modules_to_save: additional modules to train and save as lora weights\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, inference_mode=False, r=8, target_modules={'query_key_value'}, lora_alpha=8, lora_dropout=0.0, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "# config = LoraConfig(task_type=TaskType.CAUSAL_LM, target_modules=[\"query_key_value\"], modules_to_save=[\"word_embeddings\"])\n",
    "config = LoraConfig(task_type=TaskType.CAUSAL_LM, target_modules=[\"query_key_value\"])\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建 peft model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.transformer.h.0.self_attention.query_key_value.lora_A.default.weight\n",
      "base_model.model.transformer.h.0.self_attention.query_key_value.lora_B.default.weight\n",
      "base_model.model.transformer.h.1.self_attention.query_key_value.lora_A.default.weight\n",
      "base_model.model.transformer.h.1.self_attention.query_key_value.lora_B.default.weight\n",
      "base_model.model.transformer.h.2.self_attention.query_key_value.lora_A.default.weight\n",
      "base_model.model.transformer.h.2.self_attention.query_key_value.lora_B.default.weight\n",
      "base_model.model.transformer.h.3.self_attention.query_key_value.lora_A.default.weight\n",
      "base_model.model.transformer.h.3.self_attention.query_key_value.lora_B.default.weight\n",
      "base_model.model.transformer.h.4.self_attention.query_key_value.lora_A.default.weight\n",
      "base_model.model.transformer.h.4.self_attention.query_key_value.lora_B.default.weight\n",
      "base_model.model.transformer.h.5.self_attention.query_key_value.lora_A.default.weight\n",
      "base_model.model.transformer.h.5.self_attention.query_key_value.lora_B.default.weight\n",
      "base_model.model.transformer.h.6.self_attention.query_key_value.lora_A.default.weight\n",
      "base_model.model.transformer.h.6.self_attention.query_key_value.lora_B.default.weight\n",
      "base_model.model.transformer.h.7.self_attention.query_key_value.lora_A.default.weight\n",
      "base_model.model.transformer.h.7.self_attention.query_key_value.lora_B.default.weight\n",
      "base_model.model.transformer.h.8.self_attention.query_key_value.lora_A.default.weight\n",
      "base_model.model.transformer.h.8.self_attention.query_key_value.lora_B.default.weight\n",
      "base_model.model.transformer.h.9.self_attention.query_key_value.lora_A.default.weight\n",
      "base_model.model.transformer.h.9.self_attention.query_key_value.lora_B.default.weight\n",
      "base_model.model.transformer.h.10.self_attention.query_key_value.lora_A.default.weight\n",
      "base_model.model.transformer.h.10.self_attention.query_key_value.lora_B.default.weight\n",
      "base_model.model.transformer.h.11.self_attention.query_key_value.lora_A.default.weight\n",
      "base_model.model.transformer.h.11.self_attention.query_key_value.lora_B.default.weight\n",
      "base_model.model.transformer.h.12.self_attention.query_key_value.lora_A.default.weight\n",
      "base_model.model.transformer.h.12.self_attention.query_key_value.lora_B.default.weight\n",
      "base_model.model.transformer.h.13.self_attention.query_key_value.lora_A.default.weight\n",
      "base_model.model.transformer.h.13.self_attention.query_key_value.lora_B.default.weight\n",
      "base_model.model.transformer.h.14.self_attention.query_key_value.lora_A.default.weight\n",
      "base_model.model.transformer.h.14.self_attention.query_key_value.lora_B.default.weight\n",
      "base_model.model.transformer.h.15.self_attention.query_key_value.lora_A.default.weight\n",
      "base_model.model.transformer.h.15.self_attention.query_key_value.lora_B.default.weight\n",
      "base_model.model.transformer.h.16.self_attention.query_key_value.lora_A.default.weight\n",
      "base_model.model.transformer.h.16.self_attention.query_key_value.lora_B.default.weight\n",
      "base_model.model.transformer.h.17.self_attention.query_key_value.lora_A.default.weight\n",
      "base_model.model.transformer.h.17.self_attention.query_key_value.lora_B.default.weight\n",
      "base_model.model.transformer.h.18.self_attention.query_key_value.lora_A.default.weight\n",
      "base_model.model.transformer.h.18.self_attention.query_key_value.lora_B.default.weight\n",
      "base_model.model.transformer.h.19.self_attention.query_key_value.lora_A.default.weight\n",
      "base_model.model.transformer.h.19.self_attention.query_key_value.lora_B.default.weight\n",
      "base_model.model.transformer.h.20.self_attention.query_key_value.lora_A.default.weight\n",
      "base_model.model.transformer.h.20.self_attention.query_key_value.lora_B.default.weight\n",
      "base_model.model.transformer.h.21.self_attention.query_key_value.lora_A.default.weight\n",
      "base_model.model.transformer.h.21.self_attention.query_key_value.lora_B.default.weight\n",
      "base_model.model.transformer.h.22.self_attention.query_key_value.lora_A.default.weight\n",
      "base_model.model.transformer.h.22.self_attention.query_key_value.lora_B.default.weight\n",
      "base_model.model.transformer.h.23.self_attention.query_key_value.lora_A.default.weight\n",
      "base_model.model.transformer.h.23.self_attention.query_key_value.lora_B.default.weight\n"
     ]
    }
   ],
   "source": [
    "for name, params in model.named_parameters():\n",
    "    if params.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query_key_value'}\n"
     ]
    }
   ],
   "source": [
    "print(config.target_modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "(query_key_value): lora.Linear(\n",
    "                (base_layer): Linear(in_features=2048, out_features=6144, bias=True)\n",
    "                (lora_dropout): ModuleDict(\n",
    "                  (default): Identity()\n",
    "                )\n",
    "                (lora_A): ModuleDict(\n",
    "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
    "                )\n",
    "                (lora_B): ModuleDict(\n",
    "                  (default): Linear(in_features=8, out_features=6144, bias=False)\n",
    "                )\n",
    "                (lora_embedding_A): ParameterDict()\n",
    "                (lora_embedding_B): ParameterDict()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,572,864 || all params: 1,304,684,544 || trainable%: 0.120555118647899\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(output_dir='./lora/',\n",
    "                        num_train_epochs=1,\n",
    "                        per_device_train_batch_size=8,\n",
    "                        gradient_accumulation_steps=4,\n",
    "                        per_device_eval_batch_size=8,\n",
    "                        logging_steps=10,\n",
    "                        load_best_model_at_end=True,\n",
    "                        evaluation_strategy='epoch',\n",
    "                        save_strategy='epoch',\n",
    "                        save_total_limit=1,\n",
    "                        report_to='none',\n",
    "                        )\n",
    "trainer = Trainer(model=model, args=args, train_dataset=tokenized_ds['train'], eval_dataset=tokenized_ds['test'],\n",
    "                data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/yutong/anaconda3/envs/IMP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='168' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 38/168 02:59 < 10:46, 0.20 it/s, Epoch 0.22/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=168, training_loss=2.5467050870259604, metrics={'train_runtime': 945.8245, 'train_samples_per_second': 22.717, 'train_steps_per_second': 0.178, 'total_flos': 2.4970438915915776e+16, 'train_loss': 2.5467050870259604, 'epoch': 1.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: 考试有什么技巧\n",
      "\n",
      "Assistant: 考试有什么技巧？\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline('text-generation', model=model, tokenizer=tokenizer, device=0)\n",
    "ipt = \"Human: {}\\n{}\".format('考试有什么技巧', '').strip() + \"\\n\\nAssistant: \"\n",
    "result = pipe(ipt, max_length=100, num_beams=5)\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load lora weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(ckpt)\n",
    "p_model = PeftModel.from_pretrained(model, model_id=\"./lora/checkpoint-168/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: 考试有哪些技巧？\n",
      "\n",
      "Assistant: 考试的技巧有很多，比如：\n",
      "1.多做模拟题，熟悉题型和答题思路；\n",
      "2.合理分配时间，保证每道题都做对；\n",
      "3.认真审题，排除干扰选项；\n",
      "4.正确使用答题模板，规范作答；\n",
      "5.保持良好的答题心态，避免紧张、焦虑等负面情绪影响答题。\n"
     ]
    }
   ],
   "source": [
    "ipt = tokenizer(\"Human: {}\\n{}\".format(\"考试有哪些技巧？\", \"\").strip() + \"\\n\\nAssistant: \", return_tensors=\"pt\")\n",
    "ipt = {k: v.to(p_model.device) for k, v in ipt.items()}\n",
    "print(tokenizer.decode(p_model.generate(**ipt, num_beams=5, \n",
    "                    max_length=100, repetition_penalty=1.5)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merge model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_model = p_model.merge_and_unload()\n",
    "merge_model.save_pretrained(\"path_to_save\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IMP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
