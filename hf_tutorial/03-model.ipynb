{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./figs/03_attention_mask.jpg)\n",
    "\n",
    "![](./figs/03_model_example.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./figs/03_model_head.jpg)\n",
    "\n",
    "![](./figs/03_model_head_examples.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.35.2\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "checkpoint = \"hfl/rbt3\"\n",
    "config = AutoConfig.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModel.from_pretrained(checkpoint, config=config)\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"hfl/rbt3\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"directionality\": \"bidi\",\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 3,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pooler_fc_size\": 768,\n",
       "  \"pooler_num_attention_heads\": 12,\n",
       "  \"pooler_num_fc_layers\": 3,\n",
       "  \"pooler_size_per_head\": 128,\n",
       "  \"pooler_type\": \"first_token_transform\",\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.35.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 21128\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101,  791, 1921, 1921, 3698,  679, 7231,  106,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen = \"今天天气不错!\"\n",
    "inputs = tokenizer(sen, return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## without head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'pooler_output'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.output_attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(checkpoint, output_attentions=True)\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'pooler_output', 'attentions'])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 9, 9])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch_size, num_heads, seq_len, seq_len\n",
    "outputs['attentions'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.num_attention_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9, 768])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encoded results\n",
    "# batch_size, seq_len, hidden_size\n",
    "outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sequence length (including special tokens)\n",
    "len(inputs['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/rbt3 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "cls_model = AutoModelForSequenceClassification.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = cls_model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_model.num_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`from transformers import BertForSequenceClassification`\n",
    "\n",
    "```\n",
    "class BertForSequenceClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.config = config\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        classifier_dropout = (\n",
    "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
    "        )\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/rbt3 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-0.1566, -0.1988, -0.5851,  0.1531,  0.9520, -0.3869, -0.1103,  0.2260,\n",
       "         -0.4038, -0.5205]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=10)\n",
    "cls_model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], SequenceClassifierOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        # [batch, 768] 768 = hidden_size\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        # [batch, num_labels]\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                if self.num_labels == 1:\n",
    "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "                else:\n",
    "                    loss = loss_fct(logits, labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# without head call and take the pooled output, i.e., [CLS] token\n",
    "outputs[1].shape\n",
    "# logits shape\n",
    "cls_model.classifier(outputs[1]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning: sentimental analysis\n",
    "## step 1: dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>距离川沙公路较近,但是公交指示不对,如果是\"蔡陆线\"的话,会非常麻烦.建议用别的路线.房间较...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>商务大床房，房间很大，床有2M宽，整体感觉经济实惠不错!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>早餐太差，无论去多少人，那边也不加食品的。酒店应该重视一下这个问题了。房间本身很好。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>宾馆在小街道上，不大好找，但还好北京热心同胞很多~宾馆设施跟介绍的差不多，房间很小，确实挺小...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>CBD中心,周围没什么店铺,说5星有点勉强.不知道为什么卫生间没有电吹风</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                             review\n",
       "0      1  距离川沙公路较近,但是公交指示不对,如果是\"蔡陆线\"的话,会非常麻烦.建议用别的路线.房间较...\n",
       "1      1                       商务大床房，房间很大，床有2M宽，整体感觉经济实惠不错!\n",
       "2      1         早餐太差，无论去多少人，那边也不加食品的。酒店应该重视一下这个问题了。房间本身很好。\n",
       "3      1  宾馆在小街道上，不大好找，但还好北京热心同胞很多~宾馆设施跟介绍的差不多，房间很小，确实挺小...\n",
       "4      1               CBD中心,周围没什么店铺,说5星有点勉强.不知道为什么卫生间没有电吹风"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !head -n 4 data/ChnSentiCorp_htl_all.csv\n",
    "# waring: The current process just got forked....\n",
    "#  https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning/72926996#72926996\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"data/ChnSentiCorp_htl_all.csv\")\n",
    "data = data.dropna()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class ChnSentiCorp(Dataset):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.data = pd.read_csv(\"data/ChnSentiCorp_htl_all.csv\")\n",
    "        self.data = self.data.dropna()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        label = row.label\n",
    "        text = row.review\n",
    "        return text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('房间很好，符合五星级的标准。酒店离沟口及边边街都比较近，是去九寨沟旅游的最佳选择。补充点评2008年7月21日：地震对该酒店没有任何影响。宾馆反馈2008年7月24日：谢谢您的点评,欢迎下次下榻我酒店!我们将为您提供更周到的服务及优质的产品.', 1)\n",
      "('性价比还不错。。。算是当地最好的酒店了如果是第一次去的话，可能会发现不太好找', 1)\n",
      "('打开评价页面就很不一样，很少有酒店能这么认真的作评价回复，这一点就很不错了。我们是A9上海至湖州通车那天来湖州逛逛的，定了太湖豪华房，应该算是不错，建筑比较吸引人，不过那天风大雨大就一直在房间里，可能观念的问题吧，就五星标准来说，房间足够大，浴室也做的很漂亮，但却少许多东西，感觉空落落的，床上用品也只有简单的两个枕头、一条薄被，看上去就好冷啊~应该会比较适合夏季度假吧：）由于临时有事，没有过夜就提前退房了，前台mm还很遗憾的说，下次一定要来住哦，听了蛮窝心的，我想应该会在暖和些的日子再来的：）也希望再次入住前酒店能做更好的改进，哈哈补充点评2008年1月17日：关键没有国际卫视，这一点作为五星标准比较令人不能接受：S', 1)\n",
      "('5月14日入住了这家酒店，服务确实很好，前台会亲自带领客人到房间，并介绍房间设施，得知我自带电脑，立刻把网线从房间内电脑上拆下，放在桌旁待用。晚上六点后会送果盘和小礼物，那天是香蕉和小西红柿，两个毛绒公仔。房间相较其他济南同价位的酒店显得新，很干净，位置也很好！推荐！', 1)\n",
      "('位置是非常好，就在省政府对面，离步行街也很近，但房间内的条件太一般，达不到四星。我住的是398元的，还没包早餐，最多也就值200元，不过酒店能到机场接客确实很不错。如果住一晚也值了，如果住三晚以上，还是选择别的实惠一点的酒店吧。现在我才发现，通过携程网订房价格很没优势的，都是提高了价格后再折扣，折后价格比不上其他的普通酒店，以后还是要自己直接跟别的酒店联系，省了中间环节，价格会低很多，并且条件还会更好。', 1)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import  random_split\n",
    "import torch\n",
    "torch.manual_seed(42)\n",
    "dataset = ChnSentiCorp()\n",
    "trainset, valset = random_split(dataset, lengths=[0.8, 0.2])\n",
    "for i in range(5):\n",
    "    print(trainset[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 2: dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "trainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "valloader = DataLoader(valset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('酒店很不错，性价比很好的。虽然在洛阳繁华的路段，但晚上休息非常安静，睡的很饱。同时去龙门也比较方便，有公交车，乘坐出租车去龙门24元左右就可以到达的。',\n",
       "  '听当地公安机关说其士酒店服务人员不是特别厚道，据说已经出现过案件。建议换，信阳的阳光宾馆就不错，四星，建议重新考察。',\n",
       "  '洲际酒店，携程的价格确实有竞争力。应该是沈阳最便宜的五星级酒店。1）地理位置：位于商业街太原街附近，交通方便，但门口老是堵车，建议打车的话在酒店对面下，然后过马路。2）房间：很小，但可以看到街景的不错。3)健身中心居然不给拍照，其他同类酒店没此规定4）礼宾部门口服务不好，可能因为沈阳冬天太冷，人都不出来迎接客人。5）最糟糕的一点是：叫礼宾部打辆出租车去机场，上车前还问酒店的人是否打表收费，确认后上车，结果到了机场竟开口喊价，50元的车费司机喊100！！当时因赶飞机的原因没有追究，事后投诉洲际，到现在仍未解决。相当失望！',\n",
       "  '环境很好，算得上度假胜地，带露台、电脑的房间很让人愉悦。服务真的很一般很一般，办入住手续的时候居然可以连钥匙牌都给错。我们订的是一间普通套房和一间景观标间，在标间的订单中还特别注明要带景观的（这好像是废话？结果事实证明一点都不废），总台给的房间是一间带露台的套房和一间不带露台的套房。所谓套房也就是多一个麻将室，因我们之前查到酒店自己的网站上写着“景观房有露天阳台”，我们以为朝北的客房没有露台所以做成了麻将室，也就没在意。然后晚上准备休息的时候，大堂副理过来敲门，说给错钥匙了，给的两间都是套房，要换一间标房。换给我们的是朝北不带露台的房间，还说这个区域的客房统称景观房，不是一定带露台的。我们请携程交涉，才换到一间朝南的。这里的服务人员态度都好奇怪，餐厅没有人领位，清扫工自顾自抢在客人前面出门，早上8点多服务员敲门问可不可以整理房间（双休日唉，请勿打扰灯亮着没看见？），见到客人微笑问好是不要指望的。选这个宾馆一个原因是冲着锦江的牌子，相信管理、服务会比较到位吧，结果很失望，硬件硬，软件软。',\n",
       "  '帮同事订过很多次，酒店位置很好，很方便。房间也不错。',\n",
       "  '本次入住提供的房间较为满意，服务耐心，特别是饭菜比较可口，牛排是比较专业水准的，有圣诞气氛，不错。',\n",
       "  '差劲，并没有特殊的，我订了2天的房，到第三天续房要换房，结果忘了拿抽屉里的DV机，第二天，下午电话给客房中心，说给回复情况，结果未给回复；打给大堂值班经理投诉并要求协助；投诉理由：1.不知四还是五星的宾馆换房怎么无人及时发现客人遗留物品，及及时通知客人。她的理由是换房与退房不同，房间第二天早上才有服务员进去打扫的。。。。总之，什么凯莱，其实也是很差的，不要对他寄望什么。',\n",
       "  '当地最好的酒店！但是五星级酒店不是地方标准，所以在硬件一般的情况下，还得在服务上狠下功夫，向优秀的五星级饭店看齐。',\n",
       "  '我在预定时要求房间一定要安静,但还是给了我一间靠马路的房间,并对我的要求挂断电话,我对这个四星的号称没有冬天的酒店很失望.同样是在携程订的武汉酒店,我只是要求楼层高一点,酒店没办法达到我的要求,携程主动联系我,和我解释,我心情很愉快的接受了.所以环境不是主要,服务才是王道啊!',\n",
       "  '住过很多次了，房间很大，性价比比较高。缺点是对常客的习惯没有档案不能提供更体贴的服务。房间地毯的清洁差一些。电视的可选台太少了，应该可以在房间准备一份楼下澳葡街的菜牌，这样对客人更方便，也可以成为酒店的卖点。其他方面比较满意。',\n",
       "  '从入住到离开,都很满意,下次来杭州一定还会入住的.宾馆反馈2008年6月24日：感谢您对酒店的评价，这对我们而言是最受鼓舞的客人反馈。期待您的再次光临！',\n",
       "  '房间还是比较大的。也比较整洁。这在亳州应该是很好的。跟其他内地宾馆一样，我觉得服务有待提高。打电话问总机开车到他们宾馆怎么走，他们直接告诉我们不知道。晚上8点多在2楼餐厅吃饭，点菜的小姐不是很耐烦，有些菜都没在菜单上，上菜的速度也不快。不过，感觉菜的味道还不错。',\n",
       "  '酒店位置不错，房间也比较新的，在九华山区属于比较舒适和实惠的酒店，下次来还会考虑入住。',\n",
       "  '房间很大，但是设施陈旧，还有就是预授金额不能取消，莫名就要被刷掉一分。最多四星级',\n",
       "  '1。环境不错，不远就是三峡广场，无聊可以去看看美女2。早餐，赞！3。结帐速度较慢总的来说，还是不错的',\n",
       "  '非常差，下次肯定不会入住了，强烈建议各位不要住这个酒店！说是五星级的标准建的，连三星的标准都未到！假五星，未挂牌！问前台，说游泳池改造，所以还不能参与评星。估计是不敢！我入住的是1505房。1、房间设施非常旧，桌椅掉了不少漆；2、洗手间镜子周围锈迹斑斑；3、洗手间的花洒在冲凉时竟然掉下来了；4、空调调到低温时的噪音吵的睡不着；5、前台竟然只有一个男服务员，说下班了，只有他一人值班，服务很一般；5、前台结帐竟然收一块的保险费（携程网上并未注明，服务员也未提前说明）....唯一过的去的是，礼宾部的态度还可以。硬件差，软件再好也没用。第二天一早就转到隔挂四星的波斯特（POST）了。要不是飞机晚点半夜才到，我们当晚就转了。建议携程在这个酒店的前面不要打五星！',\n",
       "  '我并没有实际前往，而是帮我经理预定的。但是我经理回来对这个酒店评价还可以，和我说NOBAD，我想应该还是可以的了。',\n",
       "  '我是7月9号晚10点多的时候入住的，房间很新，据说是跟格林豪泰是同一公司的，可能是是新开业的，酒店管理不是很规范，员工早早的都下班了，给我房间换床单被套的居然是保安，不过前台跟保安的态度都很好，位置很不错，楼下就是汉中路地铁站，太平洋百货走5分钟就可以到，值得一住！另：协程的服务乱78糟的，本来订南站的酒店因为长途汽车临时变道，致电协程要求订一个靠近新客站北广场的宾馆结果居然给我定到南广场去了，而且还是在一小区里，后面自己找到了这个酒店要求他们帮我定结果跟我说没有，最后反倒是我自己问的前台，才给开了房间，协程给的价格没有任何的折扣跟前台给的是一样的！',\n",
       "  '洒店的房间太小,设计也不合理。电梯太少,三十几层楼才只有两个电梯.早餐还需先到三楼走一条长廊后再转电梯到37楼,等一次电梯最少要花上5分钟。价格也偏贵，加上早餐及宽带的费用后，与五星级的酒店差不了多少。宾馆反馈2008年4月25日：首先需要给您更正的就是,我酒店总共有29层,而早餐在27楼,电梯一共有三个,房间有大小区别,可能您这次所住的比较小,关于宽带,08年我们已经将宽带免费,而且给予携程用户含早的优惠,希望您继续支持!!!',\n",
       "  '经朋友强烈推荐而入住此酒店，中肯地评价一下：设施还可以，服务人员态度也好，交通还算便利，价格也合适，安排了印有客人名字的欢迎单。但有几个情况必须说一下：',\n",
       "  '不知道什么原因，这个酒店目前的价格做了下调，而且周末居然有促销价。不错！这次入住的是大床房，依旧是紧促的布局，舒服的床，总体来说，还是很不错的！',\n",
       "  '房间隔音设备是个很大问题,据说酒店今年要装修,的确,不装修改善一下对不起\"四星\"以及镇江当地\"最好酒店\"的招牌;服务还是可以的.总觉得镇江当地的服务不会像上海这些地方因为见多了贵宾而显得骄横.很有亲和力房间洗漱用品是不够格的,太寒碜了.和经济型酒店的类似.用点类似\"六神\"一样的品牌不会增加多少成本;早餐/自助餐性价比不错.服务也尽心,看到苍蝇飞过,不应该.住的2522房间,大堂副理电话中问个人的总体感觉,理由是:以后可以改进.这点态度是值得称道的.给我留下了良好的印象.其实宾客就是如此.哪怕你不做改善,但语言上的\"改善\"也给了人\"努力\"的感觉.加油,很看好这个酒店!宾馆反馈2008年6月19日：尊敬的宾客，您好，真诚感谢您入住我店，并感谢您对我店服务上给予的认可。下半年饭店将进行装修改造，届时您提出的房间隔音效果差的问题将得到有效解决，洗漱用品差，饭店也正在考虑逐步更换。餐厅蝇虫问题饭店一直都在积极联系卫生防疫部门，对蝇虫进行消杀。最后再次感谢您光临我店，恭候您的下次光临。',\n",
       "  '服务很不错,房间效果也很好.但为了赶飞机,不知道早餐如何?不过下次还会去住的!',\n",
       "  '该酒店虽然名义上是五星，但我却没看过这么小气的酒店，我通过携程网预定了三千五百多的特级房，明明看到网上写得清清楚楚有赠送的饮料券每房不超过两张，可在登记入住时前台只给一张，真是郁闷！这不是纯粹虚假广告吗！！！我们是两人入住只给一张还不如不要给，前台解释说可以让另一个人买一张！！！简直是讹诈！！！',\n",
       "  '老板每次出差到北京都要订这家酒店的，对它很满意哦',\n",
       "  '这个酒店住过很多次的了，商务标间和豪华标间都住过。商务标间洗澡比豪华标间舒服，有个大花洒，不过浴缸都感觉不怎么干净，所以就没有用。酒店服务也不错。足浴的MM也不错，都挺漂亮的。早餐一般，将就着吃吃。酒店不远一条街，算是美食街吧，很多吃的，川菜特别多。总的感觉比临海另外一家四星的要好。',\n",
       "  '物有所值,但其预定车票的服务太一般,我给他们商务中心要求订次日到西安的卧铺票,除要收我30元订票费外还要收我他们订票员来回打的士钱20元,到头来也只能买到无座票,我只好不让他们订了(当然还付了20元的费),但次日我自己直接在火车站买了一张2120次到西安的卧铺票(票价80元硬卧上铺),并在上车后顺利换成软卧(当然又补了43元差',\n",
       "  '在携程网上看到锦江之星评论还不错,因6月15的飞机,6月14日在花木店住了一晚,住的是标间，虽比星级宾馆简陋一点,但给人的感觉还是比较干净,上网都方便,适合自费旅游住店，在上海来讲，经济还比较实惠，去龙阳路坐磁悬浮列车比较方便,的士起步价就到磁悬浮车站直达机场。',\n",
       "  '酒店在老市中心，交通方便，房间干净。但开办时间较早，与其他四星相比，硬件略差，周围特色饭店较少。',\n",
       "  '新楼环境还是可以的,不过貌似没什么人,中餐厅的装修还是比较陈旧.服务周到,值得下次再去',\n",
       "  '我是第三次入住了,但是给这家宾馆留言还是第一次,,这次入住我的一位朋友喝醉了,走到大厅我朋友就\"现场直播\"了~~~~~呵呵,他们有几位服务员都主动来帮我,有一位服务员还帮我把我朋友背到了房间,过后他们值班经理还打电话到我房间询问我朋友的情况,这里的服务真的挺让我感动的,我下次还会入住这家宾馆.',\n",
       "  '住过的最差的酒店之一，房间有霉味，我住的是套间，外间有个电脑，网速很慢。早餐奇差！为了和其他的人一起，凑合住吧！'),\n",
       " tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "         1, 1, 1, 1, 1, 1, 1, 0])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(enumerate(trainloader))[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batched collate function to improve the efficience for tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "checkpoint = \"hfl/rbt3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    # inputs is a dict\n",
    "    inputs = tokenizer(texts, max_length=128, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "    inputs['labels'] = torch.tensor(labels)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(trainset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "valloader = DataLoader(valset, batch_size=64, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101,  868,  711,  ...,    0,    0,    0],\n",
       "        [ 101, 1310, 4495,  ...,    0,    0,    0],\n",
       "        [ 101, 2791, 7313,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 101, 2791, 7313,  ...,    0,    0,    0],\n",
       "        [ 101, 3173, 1290,  ...,    0,    0,    0],\n",
       "        [ 101, 2769, 3221,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "        0, 1, 1, 1, 1, 1, 1, 1])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(enumerate(trainloader))[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 3: model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/rbt3 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "checkpoint = \"hfl/rbt3\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "optimizer = Adam(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 4: trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.inference_mode():\n",
    "        for batch in valloader:\n",
    "            if torch.cuda.is_available():\n",
    "                batch = {k: v.cuda() for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            correct += torch.sum(preds == batch['labels'])\n",
    "        acc = correct / len(valset)\n",
    "        return acc\n",
    "\n",
    "\n",
    "def train(num_epoch=1, log_step=100):\n",
    "\n",
    "    global_step = 0\n",
    "    for ep in range(num_epoch):\n",
    "        model.train()\n",
    "        for batch in trainloader:\n",
    "            if torch.cuda.is_available():\n",
    "                batch = {k: v.cuda() for k, v in batch.items()}\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            global_step += 1\n",
    "            if global_step % log_step == 0:\n",
    "                print(f\"global_step: {global_step:6d}, loss: {loss.item():.4f}\")\n",
    "            acc = evaluate()\n",
    "        print(f\"epoch: {ep}, acc: {acc:.4f}\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step:    100, loss: 0.3555\n",
      "epoch: 0, acc: 0.8957\n",
      "global_step:    200, loss: 0.1541\n",
      "global_step:    300, loss: 0.1175\n",
      "epoch: 1, acc: 0.8957\n",
      "global_step:    400, loss: 0.1589\n",
      "global_step:    500, loss: 0.2019\n",
      "epoch: 2, acc: 0.8925\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 我觉得这家店的菜很好吃 \n",
      " 模型结果: positive\n"
     ]
    }
   ],
   "source": [
    "sentence = \"我觉得这家店的菜很好吃\"\n",
    "id2label = {0: 'negative', 1: 'positive'}\n",
    "with torch.inference_mode():\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    preds = torch.argmax(logits, dim=-1)\n",
    "    print('input:', sentence, '\\n', '模型结果:', id2label[preds.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'positive', 'score': 0.9922996163368225}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "model.config.id2label = id2label\n",
    "pipe = pipeline('text-classification', model=model, tokenizer=tokenizer, device=0)\n",
    "pipe(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IMP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
