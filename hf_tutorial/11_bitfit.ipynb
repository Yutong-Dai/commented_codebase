{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.load_from_disk('data/alpaca_data_zh')\n",
    "datasets = ds.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2ec9eb137a74664bcae00aaa2548ceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21486 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40e19eb539014ac68b3d96a37d3b924e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5372 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ckpt = 'Langboat/bloom-1b4-zh'\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckpt)\n",
    "\n",
    "def process_function(example):\n",
    "    MAX_LENGTH = 256\n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "    instruction = example['instruction']\n",
    "    input_str = example['input']\n",
    "    instruction_input_seq = \"\\n\".join([\"Human: \" + instruction, input_str]).strip() + \"\\n\\n Assistant:\"\n",
    "    response_str = example['output'] + tokenizer.eos_token\n",
    "    tokenized_instruction_input = tokenizer(instruction_input_seq)\n",
    "    tokenized_response = tokenizer(response_str)\n",
    "    input_ids = tokenized_instruction_input['input_ids'] + tokenized_response['input_ids']\n",
    "    attention_mask = tokenized_instruction_input['attention_mask'] + tokenized_response['attention_mask']\n",
    "    labels = [-100] * len(tokenized_instruction_input['input_ids']) + tokenized_response['input_ids']\n",
    "    if len(input_ids) > MAX_LENGTH:\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}\n",
    "\n",
    "tokenized_ds = datasets.map(process_function, remove_columns=ds.column_names)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 1.30 B\n",
      "Model size: 5.21 GB\n",
      "Gradient size: 5.21 GB\n",
      "Optimizer state: 10.42 GB\n",
      "Total size: 20.85 GB\n"
     ]
    }
   ],
   "source": [
    "unit = 1000\n",
    "num_params = sum(params.numel() for params in model.parameters())\n",
    "print(f'Number of parameters: {num_params/unit**3:.2f} B')\n",
    "\n",
    "model_size = num_params * 4 / unit**3\n",
    "print(f'Model size: {model_size:.2f} GB')\n",
    "\n",
    "gradient_size = model_size\n",
    "print(f'Gradient size: {gradient_size:.2f} GB')\n",
    "\n",
    "optimizer_state = model_size * 2\n",
    "print(f'Optimizer state: {optimizer_state:.2f} GB')\n",
    "\n",
    "total_size = model_size + optimizer_state + gradient_size\n",
    "print(f'Total size: {total_size:.2f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BitFit\n",
    "only update bias terms\n",
    "\n",
    "[BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models](https://arxiv.org/abs/2106.10199)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 0.000545 B\n"
     ]
    }
   ],
   "source": [
    "num_params = 0\n",
    "for name, params in model.named_parameters():\n",
    "    if 'bias' not in name:\n",
    "        params.requires_grad = False\n",
    "    else:\n",
    "        num_params += params.numel()\n",
    "print(f'Number of trainable parameters: {num_params/unit**3:.6f} B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: 考试有什么技巧\n",
      "\n",
      "Assistant: 考试有什么技巧\n",
      "Assistant: 考试有什么技巧\n",
      "Assistant: 考试有什么技巧\n",
      "Assistant: 考试有什么技巧\n",
      "Assistant: 考试有什么技巧\n",
      "Assistant: 考试有什么技巧\n",
      "Assistant: 考试有什么技巧\n",
      "Assistant: 考试有什么技巧\n",
      "Assistant: 考试有什么技巧\n",
      "Assistant: 考试有什么技巧\n",
      "Assistant: 考试有什么技巧\n",
      "Assistant: \n"
     ]
    }
   ],
   "source": [
    "# peformance before training\n",
    "from transformers import pipeline\n",
    "pipe = pipeline('text-generation', model=model, tokenizer=tokenizer, device=0)\n",
    "ipt = \"Human: {}\\n{}\".format('考试有什么技巧', '').strip() + \"\\n\\nAssistant: \"\n",
    "result = pipe(ipt, max_length=100, num_beams=5)\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(output_dir='./bitfit/',\n",
    "                        num_train_epochs=1,\n",
    "                        per_device_train_batch_size=8,\n",
    "                        gradient_accumulation_steps=4,\n",
    "                        per_device_eval_batch_size=8,\n",
    "                        logging_steps=10,\n",
    "                        load_best_model_at_end=True,\n",
    "                        evaluation_strategy='epoch',\n",
    "                        save_strategy='epoch',\n",
    "                        save_total_limit=1,\n",
    "                        report_to='none',\n",
    "                        )\n",
    "trainer = Trainer(model=model, args=args, train_dataset=tokenized_ds['train'], eval_dataset=tokenized_ds['test'],\n",
    "                data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline('text-generation', model=model, tokenizer=tokenizer, device=0)\n",
    "ipt = \"Human: {}\\n{}\".format('考试有什么技巧', '').strip() + \"\\n\\nAssistant: \"\n",
    "result = pipe(ipt, max_length=100, num_beams=5)\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IMP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
